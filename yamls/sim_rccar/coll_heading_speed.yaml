exp_name: sim_rccar/forest_coll_heading_speed
seed: 0
log_level: debug

#################
### Algorithm ###
#################

alg:

  ### Environment ###
  env: "ForestEnv(params={'use_vel': False})"
  env_eval: "ForestEnv(params={'use_vel': False})"
  n_envs: 1 # number of training environments
  render: False
  
  ### Offpolicy data ###
  
  offpolicy:  # folder path containing .pkl files with rollouts
  num_offpolicy: # number of offpolicy datapoints to load

  
  ### Steps ###
  
  total_steps: 2.4e+5 # corresponding to number of env.step(...) calls
  sample_after_n_steps: -1
  onpolicy_after_n_steps: 4.e+3 # take random actions until this many steps is reached
  
  learn_after_n_steps: 1.e+3 # when to start training the model
  train_every_n_steps: 0.25 # number of calls to model.train per env.step (if fractional, multiple trains per step)
  eval_every_n_steps: 1.e+3 # how often to evaluate policy in env_eval
  rollouts_per_eval: 12 # how many rollouts to do in env_eval when evaluating policy
  
  update_target_after_n_steps: -1 # after which the target network can be updated
  update_target_every_n_steps: 5.e+3 # how often to update target network
  save_every_n_steps: 1.e+3 # how often to save experiment data
  log_every_n_steps: 1.e+3 # how often to print log information

  
  ### Exploration ###

  exploration_strategies:
    # endpoints: [[step, value], [step, value], ...]
    GaussianStrategy: # additive gaussian noise
      endpoints: [[0, 0.25], [8.e+4, 0.05], [24.e+4, 0.005]]
      outside_value: 0.005
    EpsilonGreedyStrategy:
      endpoints: [[0, 1.0], [1.e+3, 1.0], [8.e+4, 0.1], [16.e+4, 0.01]]
      outside_value: 0.01


  ### Replay pool
    
  batch_size: 32 # per training step
  replay_pool_size: 1.e+6
  replay_pool_sampling: uniform # <uniform/terminal>
  replay_pool_params:
    terminal:
      frac: 0.5 # fraction of batch that from an end of an episode
      
      
  ### Saving data
      
  save_rollouts: True
  save_rollouts_observations: True # False saves space
  save_env_infos: True # False saves space

##############
### Policy ###
##############

policy:
  N: 16 # label horizon
  H: 16 # model horizon
  gamma: 0.99 # discount factor
  obs_history_len: 4 # number of previous observations to concatenate (inclusive)
  
  use_target: False # target network?
  separate_target_params: True # if target network, separate parameters?
  clip_cost_target_with_dones: False

  get_action_test: # how to select actions at test time (i.e., when gathering samples)
    H: 16
    type: random # <random/lattice> action selection method
    random:
      K: 4096
    lattice:

  get_action_target: # for computing target values
    H: 16
    type: random # <random/lattice>
    random:
      K: 100
    lattice:

  image_graph: # CNN
    filters: [64, 32, 32, 32]
    kernels: [8, 4, 3, 3]
    strides: [4, 2, 2, 2]
    padding: SAME
    conv_activation: relu # <relu>
    output_activation: relu # <relu/tanh/sigmoid/softmax>
    normalizer: # <layer_norm/weight_norm/batch_norm>
    batch_norm_decay: 0.9

  observation_graph: # fully connected
    hidden_layers: [256]
    hidden_activation: relu # <relu/tanh>
    output_dim: 128 # this is the hidden size of the rnn
    output_activation: relu # <relu/tanh/sigmoid/softmax>
    normalizer: # <layer_norm/weight_norm/batch_norm>
    batch_norm_decay: 0.9

  action_graph: # fully connected
    hidden_layers: [16]
    hidden_activation: relu # <relu/tanh>
    output_dim: 16
    output_activation: relu # <relu/tanh/sigmoid/softmax>
    normalizer: # <layer_norm/weight_norm/batch_norm>
    batch_norm_decay: 0.9

  rnn_graph:
    num_cells: 1
    cell_type: mulint_lstm # <rnn/mulint_rnn/lstm/mulint_lstm>
    cell_args: # If you need to pass variables to cells
      use_layer_norm: False

  output_graph: # fully connected
    hidden_layers: [16]
    hidden_activation: relu # <relu/tanh>
    normalizer: # <layer_norm/weight_norm/batch_norm>
    batch_norm_decay: 0.9

  only_completed_episodes: False # only train with fully completed episodes?
  
  # training
  weight_decay: 1.e-6 # L2 regularization
  lr_schedule: # learning rate schedule
    endpoints: [[0, 1.e-4], [1.e+6, 1.e-4]]
    outside_value: 1.e-4
  grad_clip_norm: 10 # clip the gradient magnitude

  # device
  gpu_device: 0
  gpu_frac: 0.4

  class: GCGPolicy # <GCGPolicy> model class
  GCGPolicy: &idGCGPolicy
    # actions, yhats, bhats, values, goals are available    
    rew_fn: "((1. - yhats['coll']) * tf.cos(yhats['heading'] - goals['heading'])) + 10 * (1. - yhats['coll']) - 10 * tf.square((goals['speed'] - yhats['speed']) / goals['speed'])"

    outputs:
      - name: 'coll'
        use_yhat: True
        # pre_yhats, inputs
        yhat: "tf.nn.sigmoid(pre_yhats['coll'])"
        yhat_loss: 'xentropy'
        # rewards, goals, target_inputs, target_yhats, target_bhats, target_values
        yhat_label: "tf.cast(tf.cumsum(target_inputs['coll'], axis=1) >= 1.0, tf.float32)"
        use_bhat: False
        # pre_bhats, inputs
        bhat: 'None'
        # yhats, bhats, goals, h
        value: 'None'
        # rewards, goals, target_inputs, target_yhats, target_bhats, target_values, h
        bhat_label: 'None'
        bhat_loss: 'None' 
        clip_with_done: False
      - name: 'heading'
        use_yhat: True
        yhat: "tf.cumsum(tf.nn.tanh(pre_yhats['heading']), axis=1) * 3.14159265 + inputs['heading'][:,-1:]"
        yhat_label: "target_inputs['heading']"
        yhat_loss: 'sin_2'
        use_bhat: False
        bhat: 'None'
        value: 'None'
        value_label: 'None'
        value_loss: 'None'
        clip_with_done: True
      - name: 'speed'
        use_yhat: True
        yhat: "tf.cumsum(pre_yhats['speed'], axis=1) + inputs['speed'][:,-1:]"
        yhat_loss: 'mse'
        yhat_label: "target_inputs['speed']"
        use_bhat: False
        bhat: 'None'
        value: 'None'
        bhat_label: 'None'
        bhat_loss: 'None' 
        clip_with_done: True

